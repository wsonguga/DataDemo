#!/usr/bin/env python3

# In[0]
# -*- coding: utf-8 -*-
"""SignalProcessing for HR, SBP, and DBP prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yRIqmqTL_sExexqLrGb0c9OweDWhaehA

#### Importing Necessary Packages
"""

# Commented out IPython magic to ensure Python compatibility.
import pickle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics
sns.set()
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

# ! pip install tsfel # installing TSFEL for feature extraction

import tsfel


"""### Feature Extraction & Selection
##### **TSFEL** is a python package for extracting features from a sequential/time series data. It extracts **statistical**, **spectral**, and **temporal** features from the raw signal. While extracing, some of the features have missing values in terms of positiv or negative infinite. I replace those with median of the features.

##### After the feature extraction, I removed features if they are highly correlated and contains low variance using variance threshold technique. After dropping the unnecessary variables, the data normalized where training parameters were used to normalize both train and test data
"""

def feature_extraction(data_fe):
  cfg = tsfel.get_features_by_domain()
  features = tsfel.time_series_features_extractor(cfg, data_fe)
  features.replace([np.inf, -np.inf], np.nan, inplace=True)
  features.fillna(features.median(), inplace=True)
  return features

def feature_selection(train_data, corr_features):
  # feature extraction
  train = feature_extraction(train_data)
  # test = feature_extraction(test)

  # feature selection
  # Highly correlated features are removed
  # corr_features = tsfel.correlated_features(train)
  train.drop(corr_features, axis=1, inplace=True)
  # test.drop(corr_features, axis=1, inplace=True)

  # Remove low variance features
  selector = VarianceThreshold()
  train = selector.fit_transform(train)
  # test = selector.transform(test)

  #normalizing the data
  scaler = preprocessing.StandardScaler()
  scaler.fit(train)
  train = scaler.transform(train)
  # test = scaler.transform(test)

  return train

def features_selection(train, test):
  # feature extraction
  train = feature_extraction(train)
  test = feature_extraction(test)

  # feature selection
  # Highly correlated features are removed
  corr_features = tsfel.correlated_features(train)
  train.drop(corr_features, axis=1, inplace=True)
  test.drop(corr_features, axis=1, inplace=True)

  # Remove low variance features
  selector = VarianceThreshold()
  train = selector.fit_transform(train)
  test = selector.transform(test)

  #normalizing the data
  scaler = preprocessing.StandardScaler()
  scaler.fit(train)
  train = scaler.transform(train)
  test = scaler.transform(test)

  return train, test




#In[3]
"""### Model Implementaion
##### Multiple Linear Regression, LASSO, and Ridge regession were applied to predict HR, SBP, and DBP. Mean Absolute Error (MAE) is used as performace evaluation metric. The implementaion process is two parts: **1** I only used the train data for 5-fold cross validation for showing that the applied model generalize. **2** In the second step, I used the train data (size: 5000 * 1000) for traing the model and used the test data (size 1500*1000) for evaluating the performance in interms of MAE.
"""

def model_prediction(y_fold_train, y_fold_test):
  mae_list = []
  for i in range(5):
    X_train_f, X_test_f = features_selection(X_train_fold[i], X_test_fold[i])
    model = LinearRegression().fit(X_train_f, y_fold_train[i])
    y_pred = model.predict(X_test_f)
    mae = metrics.mean_absolute_error(y_fold_test[i], y_pred)
    mae_list.append(mae)
  mean = np.mean(mae_list)
  std = np.std(mae_list)
  return mean, std


#In[1]
"""### Data Upload
##### We mounted at google drive for uploading the data into CoLab. If you code at your local machine, please consider simply uploding data. After uploading the data, the signal and the classes are separated for model building perpose.
"""

# from google.colab import drive 
# drive.mount("/content/gdrive") # mounting at gdrive

# train = np.load(r'/content/gdrive/My Drive/UGA PostDoc Project/data/train_set.npy') # loading the train data 
# test = np.load(r'/content/gdrive/My Drive/UGA PostDoc Project/data/test_set.npy') # loading the test data 
train = np.load('./data/train_set.npy') # loading the train data 
test = np.load('./data/test_set.npy') # loading the test data 
### defining Independent and dependent variables for train data 
N = 3
signal_train = train[:, :-N]
# HR_train = train[:,-3:-2]
SBP_train = train[:,-2:-1]
DBP_train = train[:,-1:]

### defining Independent and dependent variables for test data 
signal_test = test[:, :-N]
# HR_test = test[:,-3:-2]
SBP_test = test[:,-2:-1]
DBP_test = test[:,-1:]

print(signal_train, signal_test)

print(SBP_train, DBP_train)

print(SBP_test, DBP_test)

#In[2]
"""### K-Fold 
##### We apply a 5-fold cross validation on the training data to generalize the model and then use the whole train data for training the model and then use test data for performance evaluation. Though in python there are packages- like **cvscore** where we can directly apply the CV without generating the folds. But, in this case, I applied normalization techniqe where I use the parameters of training data to normalize the test data- whihch is not possible in **cvscore** package.  
"""

import numpy as np
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, random_state= 42, shuffle = True)
X_train_fold = []
X_test_fold = []
y_train_fold_HR = []
y_test_fold_HR = []
y_train_fold_SBP = []
y_test_fold_SBP = []
y_train_fold_DBP = []
y_test_fold_DBP = []
for train_index, test_index in kf.split(signal_train):
  X_train, X_test = signal_train[train_index], signal_train[test_index]
  # y_train_HR, y_test_HR = HR_train[train_index], HR_train[test_index]
  y_train_SBP, y_test_SBP = SBP_train[train_index], SBP_train[test_index]
  y_train_DBP, y_test_DBP = DBP_train[train_index], DBP_train[test_index]
  
  X_train_fold.append(X_train)
  X_test_fold.append(X_test)

  # y_train_fold_HR.append(y_train_HR)
  # y_test_fold_HR.append(y_test_HR)

  y_train_fold_SBP.append(y_train_SBP)
  y_test_fold_SBP.append(y_test_SBP)

  y_train_fold_DBP.append(y_train_DBP)
  y_test_fold_DBP.append(y_test_DBP)

# result_HR_predition = model_prediction(y_train_fold_HR, y_test_fold_HR)
result_SBP_predition = model_prediction(y_train_fold_SBP, y_test_fold_SBP)
result_DBP_predition = model_prediction(y_train_fold_DBP, y_test_fold_DBP)

# result_HR_predition

print("result_SBP_predition: ", result_SBP_predition)

print("result_DBP_predition: ", result_DBP_predition)

"""### Performance Evaluation on TEST Data"""

X_train_f, X_test_f = features_selection(signal_train, signal_test)

# model = LinearRegression().fit(X_train_f, HR_train)
# test_y_pred = model.predict(X_test_f)
# metrics.mean_absolute_error(HR_test, test_y_pred)

# model = LinearRegression().fit(X_train_f, SBP_train)
# test_y_pred = model.predict(X_test_f)
# metrics.mean_absolute_error(SBP_test, test_y_pred)

# model = LinearRegression().fit(X_train_f, DBP_train)
# test_y_pred = model.predict(X_test_f)
# metrics.mean_absolute_error(DBP_test, test_y_pred)


# %%


# model_HR = LinearRegression().fit(X_train_f, HR_train)
# HR_pred = model_HR.predict(X_test_f)
# print(metrics.mean_absolute_error(HR_test, HR_pred))
# plt.plot(HR_pred, 'b')
# plt.plot(HR_test, 'r')
# plt.show()

model_SBP = LinearRegression().fit(X_train_f, SBP_train)
SBP_pred = model_SBP.predict(X_test_f)
print(metrics.mean_absolute_error(SBP_test, SBP_pred))
# save the model to disk
filename = 'model_SBP.sav'
pickle.dump(model_SBP, open(filename, 'wb'))

model_DBP = LinearRegression().fit(X_train_f, DBP_train)
DBP_pred = model_DBP.predict(X_test_f)
print(metrics.mean_absolute_error(DBP_test, DBP_pred))
# save the model to disk
filename = 'model_DBP.sav'
pickle.dump(model_DBP, open(filename, 'wb'))

plt.figure(figsize=(20, 8))
plt.plot(SBP_pred, 'b')
plt.plot(SBP_test, 'r')
plt.show()

plt.figure(figsize=(20, 8))
plt.plot(DBP_pred, 'b')
plt.plot(DBP_test, 'r')
plt.show()



# %%

# features_train = feature_extraction(signal_train)
# corr_features = tsfel.correlated_features(features_train)

# # %%
# model_SBP = pickle.load(open('model_SBP.sav', 'rb'))
# model_DBP = pickle.load(open('model_DBP.sav', 'rb'))

# test_data = np.load('./data/bad_set.npy') 
# signal_test = test[:, :-N]
# X_test_f = feature_selection(signal_test, corr_features)

# SBP_test = test_data[:,-2:-1]
# DBP_test = test_data[:,-1:]

# SBP_pred = model_SBP.predict(X_test_f)
# print(metrics.mean_absolute_error(SBP_test, SBP_pred))
# plt.plot(SBP_pred, 'b')
# plt.plot(SBP_test, 'r')
# plt.show()

# DBP_pred = model_DBP.predict(X_test_f)
# print(metrics.mean_absolute_error(DBP_test, DBP_pred))
# plt.plot(DBP_pred, 'b')
# plt.plot(DBP_test, 'r')
# plt.show()


# %%
# import numpy as np
# train_data = np.load('./data/classifier_clean_data.npy')
# test_data = np.load('./data/m_clean_data.npy')